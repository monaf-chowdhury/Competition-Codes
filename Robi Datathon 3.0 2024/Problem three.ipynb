{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In the realm of Neverland, where enchantment stretched far and wide, there lived a kind-hearted cattleman named Grassland Gus. His farm, Moo Meadows, was the sole source of the most delicious meat and milk in the kingdom. The people of Neverland would come from far and wide to procure these treasures.\n",
    "\n",
    "Some procured these items directly from the farm at wholesale rates, while others obtained them from nearby groceries at retail prices. Fresh items were available at a premium, while frozen ones were sold at standard prices.\n",
    "\n",
    "Gus packaged his dairy and meat in Enchanted Boxes. Each box held a different combination of meat and milk, and depending on their quality, some boxes were more valuable than others.\n",
    "\n",
    "To purchase these magical boxes, the denizens of Neverland used Wishing Coins, which are tokens earned through acts of kindness. Every buyer had their own unique Magic Key, which kept track of all their purchases.\n",
    "\n",
    "All exchanges of the kingdom are logged in the Enchanted Scroll, details of which are given in the file purchase.csv. The file contains records of purchases made over the last five months, including the date of purchase, the customer's magic key, the box ID purchased and purchase unit. Denizens select boxes to purchase from a list written on parchment. The dataset Boxes.csv enumerates all available boxes, including the box ID, product quality, delivery option, quantity of milk (cauldron), quantity of meat (stones) and box unit price.\n",
    "\n",
    "There is no specific train.csv for this contest. Only **\"purchase.csv\"** and **\"boxes.csv\"** are given. You have to do everything from these two files.\n",
    "\n",
    "**\"problem 3.csv\"** is given for you to predict, **\"sample submission 3.csv\"** is also there to help you about the submission template.\n",
    "\n",
    "**“problem 3.csv”** contains the Magic Keys of customers who purchased at least one box of milk and/or meat in the first 15 days of March-2019. You need to predict what quantity of meat were be purchased by them in this period. Prepare and submit as submission.csv following the the template (sample submission 3.csv).\n",
    "\n",
    "**Evaluation**\n",
    "The evaluation metric for this problem is Root Mean Squared Error(RMSE). RMSE is calculated as the square root of the average of the squared differences between predicted and actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic testing\n",
    "This code is to test out the generic theory or eda for the problem datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "purchase = pd.read_csv(\"Problem 3/purchase.csv\")\n",
    "box = pd.read_csv(\"Problem 3/boxes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping ducplicates, nan values and impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PURCHASE_DATE    2455723\n",
       "MAGIC_KEY        2455723\n",
       "BOX_ID           2455723\n",
       "BOX_COUNT        2455723\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase = purchase.dropna().drop_duplicates() # Drop NaN values and duplicates\n",
    "positive_box_count_mask = purchase['BOX_COUNT'] >= 0\n",
    "purchase = purchase[positive_box_count_mask]\n",
    "purchase['PURCHASE_DATE'] = pd.to_datetime(purchase['PURCHASE_DATE'], format='%d/%m/%Y')\n",
    "purchase = purchase.sort_values(by='PURCHASE_DATE') # Sort purchase data by purchase date in ascending order\n",
    "purchase.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. New dynamic method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_avg_time_between_purchases and feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_time_between_purchases(group):\n",
    "    if len(group) > 1:\n",
    "        return np.mean(group['PURCHASE_DATE'].diff().dt.days)\n",
    "    else:\n",
    "        return 150\n",
    "    \n",
    "def feature_extraction(purchase,grouped_df,box):\n",
    "    # Task 1: Calculate the frequency of purchases for each Magic Key within specific time intervals (bi-weekly and monthly)\n",
    "    print('1/6 Extracting Bi-Weekly and Monthly Purchase Count...')\n",
    "    biweekly_purchase_count = purchase.groupby(['MAGIC_KEY', pd.Grouper(key='PURCHASE_DATE', freq='2W')]).size().unstack(fill_value=0)\n",
    "    monthly_purchase_count = purchase.groupby(['MAGIC_KEY', pd.Grouper(key='PURCHASE_DATE', freq='ME')]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Task 2: Calculate the average time between purchases for each Magic Key\n",
    "    print('2/6 Extracting Average Time between purchase...')\n",
    "    # avg_time_between_purchases = grouped_df.apply(calculate_avg_time_between_purchases) \n",
    "    avg_time_between_purchases = 0\n",
    "\n",
    "    # Task 3 days_since_last_purchase\n",
    "    print('3/6 Extracting Days since last purchase...')\n",
    "    last_purchase_date = grouped_df['PURCHASE_DATE'].max()\n",
    "    days_since_last_purchase = (purchase['PURCHASE_DATE'].max() - last_purchase_date).dt.days.copy()\n",
    "    \n",
    "    # Task 4 purchase_count and total_spent\n",
    "    print('4/6 Extracting purchase_count and total_spent...')\n",
    "    merged_df = pd.merge(purchase, box, on='BOX_ID') \n",
    "    merged_df['SPENT'] = merged_df['BOX_COUNT'] * merged_df['UNIT_PRICE']\n",
    "    grouped_df = merged_df.groupby('MAGIC_KEY') \n",
    "    purchase_count = grouped_df.size().rename('Purchase_Count') \n",
    "    total_spent = grouped_df['SPENT'].sum().rename('Total_Spent')\n",
    "    # total_spent = grouped_df['UNIT_PRICE'].sum().rename('Total_Spent')\n",
    "\n",
    "    # Task 5  total_milk_quantity & total_meat_quantity\n",
    "    print('5/6 Extracting total_milk_quantity & total_meat_quantity...')\n",
    "    total_milk_quantity = grouped_df['MILK'].sum().rename('Total_Milk_Quantity')\n",
    "    total_meat_quantity = grouped_df['MEAT'].sum().rename('Total_Meat_Quantity')\n",
    "    \n",
    "    # Task 6 num_purchases_first_15_days and num_purchases_last_15_days\n",
    "    # I will give it a try by dropping them too\n",
    "    print('6/6 Extracting num_purchases_first_15_days and num_purchases_last_15_days...')\n",
    "    first_15_days_purchase = merged_df[merged_df['PURCHASE_DATE'].dt.day <= 15]\n",
    "    num_purchases_first_15_days = first_15_days_purchase.groupby(['MAGIC_KEY', first_15_days_purchase['PURCHASE_DATE'].dt.month]).size().groupby('MAGIC_KEY').sum()\n",
    "    last_15_days_purchase = merged_df[merged_df['PURCHASE_DATE'].dt.day > 15]\n",
    "    num_purchases_last_15_days = last_15_days_purchase.groupby(['MAGIC_KEY', last_15_days_purchase['PURCHASE_DATE'].dt.month]).size().groupby('MAGIC_KEY').sum()\n",
    "    \n",
    "    # Task 7 Most common box \n",
    "    print('$$ Extracting Most common box...')\n",
    "    most_common_boxes = merged_df.groupby('MAGIC_KEY')['BOX_ID'].agg(lambda x: x.mode()[0])\n",
    "\n",
    "\n",
    "    # Combine all features into a DataFrame\n",
    "    features = pd.DataFrame({\n",
    "        'Biweekly_Purchase_Count': biweekly_purchase_count.mean(axis=1),\n",
    "        'Monthly_Purchase_Count': monthly_purchase_count.mean(axis=1),\n",
    "        'Avg_Time_Between_Purchases': avg_time_between_purchases,\n",
    "        'Days_Since_Last_Purchase': days_since_last_purchase\n",
    "    })\n",
    "    purchase_history_features = pd.concat([purchase_count, total_spent], axis=1) # Create a new DataFrame with purchase history features\n",
    "    features = features.join(purchase_history_features, how='left')\n",
    "    box_features_df = pd.concat([total_milk_quantity, total_meat_quantity], axis=1)\n",
    "    features = features.join(box_features_df, how='left')\n",
    "    features['Num_Purchases_First_15_Days'] = num_purchases_first_15_days     # Dropping 2 features \n",
    "    features['Num_Purchases_Last_15_Days'] = num_purchases_last_15_days       # Dropping 2 features\n",
    "    features['Most_common_box'] = most_common_boxes\n",
    "    features = features.fillna(0)\n",
    "    features = features.drop(columns=['Avg_Time_Between_Purchases'])          # Dropping avg time between purchases \n",
    "    return features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set time boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 Extracting Bi-Weekly and Monthly Purchase Count...\n",
      "2/6 Extracting Average Time between purchase...\n",
      "3/6 Extracting Days since last purchase...\n",
      "4/6 Extracting purchase_count and total_spent...\n",
      "5/6 Extracting total_milk_quantity & total_meat_quantity...\n",
      "6/6 Extracting num_purchases_first_15_days and num_purchases_last_15_days...\n",
      "$$ Extracting Most common box...\n"
     ]
    }
   ],
   "source": [
    "purchase_oct_nov = purchase[(purchase['PURCHASE_DATE'].dt.year == 2018) &\n",
    "                                    ((purchase['PURCHASE_DATE'].dt.month == 10) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 11))]\n",
    "grouped_df_oct_nov = purchase_oct_nov.groupby('MAGIC_KEY') # Group by MAGIC_KEY\n",
    "features = feature_extraction(purchase_oct_nov,grouped_df_oct_nov,box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean/median used labelling features  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "def Labelling_features(features, purchase, box , end_date):\n",
    "\n",
    "    start_date = pd.Timestamp('2018-10-01')\n",
    "    end_date = pd.Timestamp(end_date)\n",
    "\n",
    "    merged_df = pd.merge(purchase, box, on='BOX_ID')\n",
    "    merged_df = merged_df[(merged_df['PURCHASE_DATE'] >= start_date) & (merged_df['PURCHASE_DATE'] <= end_date)]\n",
    "\n",
    "    most_common_meat = merged_df.groupby('MAGIC_KEY')['MEAT'].median()\n",
    "    most_common_meat_df = most_common_meat.to_frame(name='Most_Common_Box')\n",
    "\n",
    "    features_with_labels = pd.merge(features, most_common_meat_df, left_index=True, right_index=True, how='left')\n",
    "    features_with_labels = features_with_labels.dropna(subset=['Most_Common_Box'])\n",
    "    features_with_labels = features_with_labels.rename(columns={'Most_Common_Box': 'labels'})\n",
    "\n",
    "    return features_with_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = Labelling_features(features,purchase, 2018,12) # for december of 2018\n",
    "\n",
    "features = Labelling_features(features,purchase, box, '2018-12-15') # for december of 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "This was used for final submission. Three step solution \n",
    "* Using data from OCT-NOV           ; Labels = DEC Half\n",
    "* Using data from OCT-NOV-DEC       ; Labels = JAN Half\n",
    "* Using data from OCT-NOV-DEC-JAN   ; Labels = FEB Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.12039738479941026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "# using RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "labels = features['labels'].to_numpy()\n",
    "features_pure = (features.drop(columns=['labels'])).to_numpy()\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pure)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.25, random_state=42)\n",
    "classifier = RandomForestRegressor(verbose = 1)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    " \n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 Extracting Bi-Weekly and Monthly Purchase Count...\n",
      "2/6 Extracting Average Time between purchase...\n",
      "3/6 Extracting Days since last purchase...\n",
      "4/6 Extracting purchase_count and total_spent...\n",
      "5/6 Extracting total_milk_quantity & total_meat_quantity...\n",
      "6/6 Extracting num_purchases_first_15_days and num_purchases_last_15_days...\n",
      "$$ Extracting Most common box...\n",
      "Feature Labelling is going on...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.1106643415604782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Model training for oct_nov_dec\n",
    "purchase_oct_nov_dec = purchase[(purchase['PURCHASE_DATE'].dt.year == 2018) &\n",
    "                                    ((purchase['PURCHASE_DATE'].dt.month == 10) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 11) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 12))]\n",
    "grouped_df_oct_nov_dec = purchase_oct_nov_dec.groupby('MAGIC_KEY') # Group by MAGIC_KEY\n",
    "features = feature_extraction(purchase_oct_nov_dec,grouped_df_oct_nov_dec,box)\n",
    "\n",
    "# Labelling \n",
    "print(\"Feature Labelling is going on...\")\n",
    "# features = Labelling_features(features,purchase, 2019, 1) # for  of 2019\n",
    "features = Labelling_features(features,purchase, box, '2019-01-15') # for January of 2019\n",
    "\n",
    "#defining elimentary things\n",
    "labels = features['labels'].to_numpy()\n",
    "features_pure = (features.drop(columns=['labels'])).to_numpy()\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pure)\n",
    "\n",
    "# Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.25, random_state=42)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    " \n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 Extracting Bi-Weekly and Monthly Purchase Count...\n",
      "2/6 Extracting Average Time between purchase...\n",
      "3/6 Extracting Days since last purchase...\n",
      "4/6 Extracting purchase_count and total_spent...\n",
      "5/6 Extracting total_milk_quantity & total_meat_quantity...\n",
      "6/6 Extracting num_purchases_first_15_days and num_purchases_last_15_days...\n",
      "$$ Extracting Most common box...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.11113453094235855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    7.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Model training for oct_nov_dec_jan\n",
    "# Time boundary, OCT, Nov, Dec, Jan\n",
    "purchase_oct_nov_dec_jan = purchase[((purchase['PURCHASE_DATE'].dt.year == 2018) | (purchase['PURCHASE_DATE'].dt.year == 2019)) &\n",
    "                                    ((purchase['PURCHASE_DATE'].dt.month == 10) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 11) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 12) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 1))]\n",
    "grouped_df_oct_nov_dec_jan = purchase_oct_nov_dec_jan.groupby('MAGIC_KEY') # Group by MAGIC_KEY\n",
    "features = feature_extraction(purchase_oct_nov_dec_jan,grouped_df_oct_nov_dec_jan,box)\n",
    "\n",
    "# Labelling \n",
    "# features = Labelling_features(features,purchase, 2019, 2) # for February of 2019\n",
    "features = Labelling_features(features,purchase,box, '2019-02-15') # for February of 2019\n",
    "\n",
    "#defining elimentary things\n",
    "labels = features['labels'].to_numpy()\n",
    "features_pure = (features.drop(columns=['labels'])).to_numpy()\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pure)\n",
    "\n",
    "# Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.25, random_state=42)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    " \n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model defined \n",
    "Although this model wasn't used for final submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def define_model(n_input, n_hidden_layer, n_unit_per_layer,dropout_rate=0.25, l2_penalty=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_unit_per_layer, input_shape=(n_input,), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(l2_penalty)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # # Adding one additional hidden layer\n",
    "    # model.add(Dense(n_unit_per_layer, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(l2_penalty)))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "    \n",
    "\n",
    "    model.add(Dense(1, activation='linear'))  # 1 class, so output layer has 290 units and uses softmax activation\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train on OCT-NOV data. label DEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = features['labels'].to_numpy()\n",
    "features_pure = (features.drop(columns=['labels'])).to_numpy()\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pure)\n",
    "num_node = 50\n",
    "\n",
    "model_oct_nov = define_model(features_scaled.shape[1], 1, num_node)\n",
    "model_oct_nov.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])  # Using Mean Squared Error as the loss function\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.25, random_state=42)\n",
    "hist_oct_nov = model_oct_nov.fit(X_train, y_train, epochs=50, batch_size=4096)\n",
    "test_loss, test_accuracy = model_oct_nov.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "model_oct_nov_weights = model_oct_nov.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_oct_nov.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train on OCT-NOV-DEC Label Jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time boundary, OCT, Nov, Dec\n",
    "purchase_oct_nov_dec = purchase[(purchase['PURCHASE_DATE'].dt.year == 2018) &\n",
    "                                    ((purchase['PURCHASE_DATE'].dt.month == 10) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 11) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 12))]\n",
    "grouped_df_oct_nov_dec = purchase_oct_nov_dec.groupby('MAGIC_KEY') # Group by MAGIC_KEY\n",
    "features = feature_extraction(purchase_oct_nov_dec,grouped_df_oct_nov_dec,box)\n",
    "\n",
    "# Labelling \n",
    "print(\"Feature Labelling is going on...\")\n",
    "# features = Labelling_features(features,purchase, 2019, 1) # for  of 2019\n",
    "features = Labelling_features(features,purchase, '2019-01-15') # for January of 2019\n",
    "\n",
    "#defining elimentary things\n",
    "labels = features['labels'].to_numpy()\n",
    "features_pure = (features.drop(columns=['labels'])).to_numpy()\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model defining and setting previous weights\n",
    "model_oct_nov_dec = define_model(features_scaled.shape[1], 1, num_node)\n",
    "model_oct_nov_dec.set_weights(model_oct_nov_weights)\n",
    "model_oct_nov_dec.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)\n",
    "hist_oct_nov_dec = model_oct_nov_dec.fit(X_train, y_train, epochs=100, batch_size=4096)\n",
    "test_loss, test_accuracy = model_oct_nov_dec.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "model_oct_nov_dec_weights = model_oct_nov_dec.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_oct_nov_dec.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train on OCT-NOV-DEC-Jan Label Feb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time boundary, OCT, Nov, Dec, Jan\n",
    "purchase_oct_nov_dec_jan = purchase[((purchase['PURCHASE_DATE'].dt.year == 2018) | (purchase['PURCHASE_DATE'].dt.year == 2019)) &\n",
    "                                    ((purchase['PURCHASE_DATE'].dt.month == 10) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 11) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 12) |\n",
    "                                     (purchase['PURCHASE_DATE'].dt.month == 1))]\n",
    "grouped_df_oct_nov_dec_jan = purchase_oct_nov_dec_jan.groupby('MAGIC_KEY') # Group by MAGIC_KEY\n",
    "features = feature_extraction(purchase_oct_nov_dec_jan,grouped_df_oct_nov_dec_jan,box)\n",
    "\n",
    "# Labelling \n",
    "# features = Labelling_features(features,purchase, 2019, 2) # for February of 2019\n",
    "features = Labelling_features(features,purchase, '2019-02-15') # for February of 2019\n",
    "\n",
    "#defining elimentary things\n",
    "labels = features['labels'].to_numpy()\n",
    "features_pure = (features.drop(columns=['labels'])).to_numpy()\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model defining and setting previous weights\n",
    "model_oct_nov_dec_jan = define_model(features_scaled.shape[1], 1, num_node)\n",
    "model_oct_nov_dec_jan.set_weights(model_oct_nov_dec_weights)\n",
    "model_oct_nov_dec_jan.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)\n",
    "hist_oct_nov_dec_jan = model_oct_nov_dec_jan.fit(X_train, y_train, epochs=100, batch_size=4096)\n",
    "test_loss, test_accuracy = model_oct_nov_dec_jan.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "model_oct_nov_dec_jan_weights = model_oct_nov_dec_jan.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_oct_nov_dec_jan.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is your TensorFlow model object\n",
    "\n",
    "model_oct_nov_dec_jan.save_weights('model.weights.h5')\n",
    "\n",
    "# model = define_model(features_scaled.shape[1], 1, num_node)\n",
    "# model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Feb test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAGIC_KEY</th>\n",
       "      <th>MEAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1867906</th>\n",
       "      <td>2918AD53FC5</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867907</th>\n",
       "      <td>28F9F1C4022</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867908</th>\n",
       "      <td>28F136BD796</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867909</th>\n",
       "      <td>2CEF3F9E019</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867910</th>\n",
       "      <td>2BE8E1C3686</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164942</th>\n",
       "      <td>2C7E8FEB228</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164943</th>\n",
       "      <td>2C08F604F7D</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164944</th>\n",
       "      <td>2CC80E6BA25</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164945</th>\n",
       "      <td>2BC9D82492E</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164946</th>\n",
       "      <td>2CE9D133599</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297041 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MAGIC_KEY  MEAT\n",
       "1867906  2918AD53FC5   2.4\n",
       "1867907  28F9F1C4022   3.6\n",
       "1867908  28F136BD796   3.6\n",
       "1867909  2CEF3F9E019   2.9\n",
       "1867910  2BE8E1C3686   1.8\n",
       "...              ...   ...\n",
       "2164942  2C7E8FEB228   2.0\n",
       "2164943  2C08F604F7D   2.9\n",
       "2164944  2CC80E6BA25   3.3\n",
       "2164945  2BC9D82492E   0.0\n",
       "2164946  2CE9D133599   0.0\n",
       "\n",
       "[297041 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter data for the first 15 days of February 2019\n",
    "start_date = pd.Timestamp('2019-02-01')\n",
    "end_date = pd.Timestamp('2019-02-15')\n",
    "merged_df = pd.merge(purchase, box, on='BOX_ID')\n",
    "merged_df = merged_df[(merged_df['PURCHASE_DATE'] >= start_date) & (merged_df['PURCHASE_DATE'] <= end_date)]\n",
    "selected_columns = ['MAGIC_KEY', 'MEAT']\n",
    "feb_test = merged_df[selected_columns]\n",
    "feb_test\n",
    "# most_common_meat = merged_df.groupby('MAGIC_KEY')['MEAT'].median()\n",
    "# most_common_meat_df = most_common_meat.to_frame(name='Most_Common_Box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 Extracting Bi-Weekly and Monthly Purchase Count...\n",
      "2/6 Extracting Average Time between purchase...\n",
      "3/6 Extracting Days since last purchase...\n",
      "4/6 Extracting purchase_count and total_spent...\n",
      "5/6 Extracting total_milk_quantity & total_meat_quantity...\n",
      "6/6 Extracting num_purchases_first_15_days and num_purchases_last_15_days...\n",
      "$$ Extracting Most common box...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Biweekly_Purchase_Count</th>\n",
       "      <th>Monthly_Purchase_Count</th>\n",
       "      <th>Days_Since_Last_Purchase</th>\n",
       "      <th>Purchase_Count</th>\n",
       "      <th>Total_Spent</th>\n",
       "      <th>Total_Milk_Quantity</th>\n",
       "      <th>Total_Meat_Quantity</th>\n",
       "      <th>Num_Purchases_First_15_Days</th>\n",
       "      <th>Num_Purchases_Last_15_Days</th>\n",
       "      <th>Most_common_box</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAGIC_KEY</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2918AD53FC5</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>26.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28F9F1C4022</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>31.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28F136BD796</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>47.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2CEF3F9E019</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>107.90</td>\n",
       "      <td>84.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2BE8E1C3686</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>59.94</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C7E8FEB228</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>99.90</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C08F604F7D</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>103.90</td>\n",
       "      <td>68.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2CC80E6BA25</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>79.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2BC9D82492E</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>2.2</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>151.96</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2CE9D133599</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.4</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>85.86</td>\n",
       "      <td>70.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297041 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Biweekly_Purchase_Count  Monthly_Purchase_Count  \\\n",
       "MAGIC_KEY                                                      \n",
       "2918AD53FC5                 0.166667                     0.4   \n",
       "28F9F1C4022                 0.166667                     0.4   \n",
       "28F136BD796                 0.250000                     0.6   \n",
       "2CEF3F9E019                 0.416667                     1.0   \n",
       "2BE8E1C3686                 0.250000                     0.6   \n",
       "...                              ...                     ...   \n",
       "2C7E8FEB228                 0.416667                     1.0   \n",
       "2C08F604F7D                 0.416667                     1.0   \n",
       "2CC80E6BA25                 0.416667                     1.0   \n",
       "2BC9D82492E                 0.916667                     2.2   \n",
       "2CE9D133599                 0.583333                     1.4   \n",
       "\n",
       "             Days_Since_Last_Purchase  Purchase_Count  Total_Spent  \\\n",
       "MAGIC_KEY                                                            \n",
       "2918AD53FC5                        27               2        26.14   \n",
       "28F9F1C4022                        27               2        31.92   \n",
       "28F136BD796                         4               3        47.88   \n",
       "2CEF3F9E019                         6               5       107.90   \n",
       "2BE8E1C3686                        27               3        59.94   \n",
       "...                               ...             ...          ...   \n",
       "2C7E8FEB228                        13               5        99.90   \n",
       "2C08F604F7D                        13               5       103.90   \n",
       "2CC80E6BA25                        13               5        79.80   \n",
       "2BC9D82492E                        13              11       151.96   \n",
       "2CE9D133599                        13               7        85.86   \n",
       "\n",
       "             Total_Milk_Quantity  Total_Meat_Quantity  \\\n",
       "MAGIC_KEY                                               \n",
       "2918AD53FC5                 10.0                  2.4   \n",
       "28F9F1C4022                  0.0                  7.2   \n",
       "28F136BD796                  0.0                 10.8   \n",
       "2CEF3F9E019                 84.0                 11.2   \n",
       "2BE8E1C3686                 35.0                  5.6   \n",
       "...                          ...                  ...   \n",
       "2C7E8FEB228                 80.0                  9.4   \n",
       "2C08F604F7D                 68.0                 10.1   \n",
       "2CC80E6BA25                  0.0                 16.5   \n",
       "2BC9D82492E                115.0                  0.0   \n",
       "2CE9D133599                 70.5                  0.0   \n",
       "\n",
       "             Num_Purchases_First_15_Days  Num_Purchases_Last_15_Days  \\\n",
       "MAGIC_KEY                                                              \n",
       "2918AD53FC5                          1.0                         1.0   \n",
       "28F9F1C4022                          1.0                         1.0   \n",
       "28F136BD796                          2.0                         1.0   \n",
       "2CEF3F9E019                          4.0                         1.0   \n",
       "2BE8E1C3686                          3.0                         0.0   \n",
       "...                                  ...                         ...   \n",
       "2C7E8FEB228                          1.0                         4.0   \n",
       "2C08F604F7D                          1.0                         4.0   \n",
       "2CC80E6BA25                          2.0                         3.0   \n",
       "2BC9D82492E                          6.0                         5.0   \n",
       "2CE9D133599                          2.0                         5.0   \n",
       "\n",
       "             Most_common_box  \n",
       "MAGIC_KEY                     \n",
       "2918AD53FC5            233.0  \n",
       "28F9F1C4022            255.0  \n",
       "28F136BD796            255.0  \n",
       "2CEF3F9E019            143.0  \n",
       "2BE8E1C3686             42.0  \n",
       "...                      ...  \n",
       "2C7E8FEB228            143.0  \n",
       "2C08F604F7D            137.0  \n",
       "2CC80E6BA25            106.0  \n",
       "2BC9D82492E             89.0  \n",
       "2CE9D133599             89.0  \n",
       "\n",
       "[297041 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = purchase.groupby('MAGIC_KEY') # Group by MAGIC_KEY\n",
    "features = feature_extraction(purchase,grouped_df,box)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "extracted_features_df = features.loc[feb_test['MAGIC_KEY']]\n",
    "extracted_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.41284149, 3.2655    , 3.126     , ..., 4.545     , 0.017     ,\n",
       "       0.02      ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suitable for neural networks\n",
    "X_sub = scaler.transform(extracted_features_df)\n",
    "predictions = classifier.predict(X_sub)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.42746913436016526\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(feb_test['MEAT'].values, predictions))\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Biweekly_Purchase_Count</th>\n",
       "      <th>Monthly_Purchase_Count</th>\n",
       "      <th>Days_Since_Last_Purchase</th>\n",
       "      <th>Purchase_Count</th>\n",
       "      <th>Total_Spent</th>\n",
       "      <th>Total_Milk_Quantity</th>\n",
       "      <th>Total_Meat_Quantity</th>\n",
       "      <th>Num_Purchases_First_15_Days</th>\n",
       "      <th>Num_Purchases_Last_15_Days</th>\n",
       "      <th>Most_common_box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2BCFE9C06A7</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>240.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C2A872B5A2</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.96</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C6A897671B</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>71.94</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C6F1287F53</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>431.94</td>\n",
       "      <td>102.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C658198CC9</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>71.94</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C0804EFE49</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C080B48630</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.18</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C08243C58E</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C082C78575</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.98</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2C083B1F3E5</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5379 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Biweekly_Purchase_Count  Monthly_Purchase_Count  \\\n",
       "2BCFE9C06A7                 0.416667                     1.0   \n",
       "2C2A872B5A2                 0.166667                     0.4   \n",
       "2C6A897671B                 0.250000                     0.6   \n",
       "2C6F1287F53                 0.833333                     2.0   \n",
       "2C658198CC9                 0.250000                     0.6   \n",
       "...                              ...                     ...   \n",
       "2C0804EFE49                 0.083333                     0.2   \n",
       "2C080B48630                 0.083333                     0.2   \n",
       "2C08243C58E                 0.083333                     0.2   \n",
       "2C082C78575                 0.083333                     0.2   \n",
       "2C083B1F3E5                 0.166667                     0.4   \n",
       "\n",
       "             Days_Since_Last_Purchase  Purchase_Count  Total_Spent  \\\n",
       "2BCFE9C06A7                       0.0             5.0       240.00   \n",
       "2C2A872B5A2                      86.0             2.0        43.96   \n",
       "2C6A897671B                       0.0             3.0        71.94   \n",
       "2C6F1287F53                       0.0            10.0       431.94   \n",
       "2C658198CC9                      24.0             3.0        71.94   \n",
       "...                               ...             ...          ...   \n",
       "2C0804EFE49                      75.0             1.0        11.96   \n",
       "2C080B48630                      95.0             1.0        12.18   \n",
       "2C08243C58E                      28.0             1.0        11.96   \n",
       "2C082C78575                     126.0             1.0        12.98   \n",
       "2C083B1F3E5                      18.0             2.0        27.92   \n",
       "\n",
       "             Total_Milk_Quantity  Total_Meat_Quantity  \\\n",
       "2BCFE9C06A7                 50.0                 18.0   \n",
       "2C2A872B5A2                 20.0                  4.0   \n",
       "2C6A897671B                 31.0                  6.6   \n",
       "2C6F1287F53                102.0                 31.8   \n",
       "2C658198CC9                 31.0                  6.6   \n",
       "...                          ...                  ...   \n",
       "2C0804EFE49                  0.0                  2.5   \n",
       "2C080B48630                 10.0                  0.0   \n",
       "2C08243C58E                  0.0                  2.4   \n",
       "2C082C78575                 10.0                  1.8   \n",
       "2C083B1F3E5                  0.0                  5.3   \n",
       "\n",
       "             Num_Purchases_First_15_Days  Num_Purchases_Last_15_Days  \\\n",
       "2BCFE9C06A7                          0.0                         5.0   \n",
       "2C2A872B5A2                          2.0                         0.0   \n",
       "2C6A897671B                          0.0                         3.0   \n",
       "2C6F1287F53                          2.0                         8.0   \n",
       "2C658198CC9                          3.0                         0.0   \n",
       "...                                  ...                         ...   \n",
       "2C0804EFE49                          1.0                         0.0   \n",
       "2C080B48630                          0.0                         1.0   \n",
       "2C08243C58E                          0.0                         1.0   \n",
       "2C082C78575                          0.0                         1.0   \n",
       "2C083B1F3E5                          1.0                         1.0   \n",
       "\n",
       "             Most_common_box  \n",
       "2BCFE9C06A7             56.0  \n",
       "2C2A872B5A2            132.0  \n",
       "2C6A897671B            162.0  \n",
       "2C6F1287F53             56.0  \n",
       "2C658198CC9            162.0  \n",
       "...                      ...  \n",
       "2C0804EFE49             77.0  \n",
       "2C080B48630              8.0  \n",
       "2C08243C58E              3.0  \n",
       "2C082C78575             17.0  \n",
       "2C083B1F3E5             27.0  \n",
       "\n",
       "[5379 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('Problem 3/sample submission 3.csv')\n",
    "problem = pd.read_csv('Problem 3/problem 3.csv')\n",
    "\n",
    "mean_values = features.mean()\n",
    "new_row = pd.DataFrame([mean_values], index=['290D33249B7'], columns=features.columns)\n",
    "features = pd.concat([features, new_row])\n",
    "\n",
    "extracted_features_df = features.loc[problem['MAGIC_KEY']]\n",
    "extracted_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5379,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suitable for neural networks\n",
    "X_sub = scaler.transform(extracted_features_df)\n",
    "predictions = classifier.predict(X_sub)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5379, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with MAGIC_KEY and PURCHASE columns\n",
    "submit = pd.DataFrame({'MAGIC_KEY': problem['MAGIC_KEY'], 'MEAT': predictions})\n",
    "submit.to_csv('submit_p3_v3_.csv', index=False) \n",
    "submit.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
